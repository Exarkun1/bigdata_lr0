{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXSo7b3MGW2T"
      },
      "source": [
        "<br>\n",
        "    <center>МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ</center>\n",
        "    <center>федеральное государственное автономное образовательное учреждение высшего образования </center> <center>«Самарский национальный исследовательский университет имени академика С.П. Королева»</center>\n",
        "    <center>(Самарский университет)</center> </br>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<br>\n",
        "<center>Институт \t     информатики и кибернетики</center>                                                   \t  \n",
        "<center>Кафедра \t     технической кибернетики</center>\n",
        "</br>\n",
        "<br/>\n",
        "\n",
        "<br>\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "<br/>\n",
        "<br>\n",
        "<center>ОТЧЕТ</center>\n",
        "<center>по лабораторной работе №1</center>\n",
        "\n",
        "<center>«Введение в MapReduce модель на Python»</center>\n",
        "<br/>\n",
        "<center>по дисциплине <strong>«Большие данные»</strong></center>\n",
        "<br/>\n",
        "<center></center>\n",
        "</br>\n",
        "<br/>\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "<p style=\"text-align:right;\">Выполнил: Казаков П.А.\n",
        "<br>6132-010402D\n",
        "<br>    \n",
        "<br>Преподаватель: Попов С.Б.\n",
        "</p>\n",
        "<br/>\n",
        "<br/>\n",
        "<br/>\n",
        "    <br/>\n",
        "<br/>\n",
        "<br/>\n",
        "<center>Самара 2025</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82OvPKEiEqjc"
      },
      "source": [
        "# Введение в MapReduce модель на Python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq3EWRIpwSiJ"
      },
      "source": [
        "# **Модель MapReduce**\n",
        "Функция MapReduce имитирует работу фреймворка MapReduce: подготовка и форматирование исходных данных в виде набора пар ключ-значение (key-value) для последующего вызова функции MAP, вызов функции MAP, группировка промежуточных результатов работы функции MAP по ключу и формирование массива значений для каждого ключа, вызов функции REDUCE.\n",
        "\n",
        "Пользователь для решения своей задачи реализует функции RECORDREADER, MAP, REDUCE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1PZeQMwwVjc"
      },
      "outputs": [],
      "source": [
        "def flatten(nested_iterable):\n",
        "  for iterable in nested_iterable:\n",
        "    for element in iterable:\n",
        "      yield element\n",
        "\n",
        "def groupbykey(iterable):\n",
        "  t = {}\n",
        "  for (k2, v2) in iterable:\n",
        "    t[k2] = t.get(k2, []) + [v2]\n",
        "  return t.items()\n",
        "\n",
        "def MapReduce(RECORDREADER, MAP, REDUCE):\n",
        "  return flatten(map(lambda x: REDUCE(*x), groupbykey(flatten(map(lambda x: MAP(*x), RECORDREADER())))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFIVrimep678"
      },
      "source": [
        "## Спецификация MapReduce\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "f (k1, v1) -> (k2,v2)*\n",
        "g (k2, v2*) -> (k3,v3)*\n",
        "\n",
        "mapreduce ((k1,v1)*) -> (k3,v3)*\n",
        "groupby ((k2,v2)*) -> (k2,v2*)*\n",
        "flatten (e2**) -> e2*\n",
        "\n",
        "mapreduce .map(f).flatten.groupby(k2).map(g).flatten\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7az-6DA6qr2"
      },
      "source": [
        "## WordCount (пример)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN-nbtgG6uYG",
        "outputId": "03c69e57-b716-43ae-f6df-5f0f1fa66bf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('it', 9), ('is', 9), ('what', 5), ('a', 1), ('banana', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "from typing import Iterator\n",
        "\n",
        "d1 = \"\"\"it is what it is\n",
        "it is what it is\n",
        "it is what it is\"\"\"\n",
        "d2 = \"\"\"what is it\n",
        "what is it\"\"\"\n",
        "d3 = \"\"\"it is a banana\"\"\"\n",
        "documents = [d1, d2, d3]\n",
        "\n",
        "def RECORDREADER():\n",
        "  for (docid, document) in enumerate(documents):\n",
        "    for (lineid, line) in enumerate(document.split('\\n')):\n",
        "      yield (\"{}:{}\".format(docid,lineid), line)\n",
        "\n",
        "def MAP(docId:str, line:str):\n",
        "  for word in line.split(\" \"):\n",
        "    yield (word, 1)\n",
        "\n",
        "def REDUCE(word:str, counts:Iterator[int]):\n",
        "  sum = 0\n",
        "  for c in counts:\n",
        "    sum += c\n",
        "  yield (word, sum)\n",
        "\n",
        "output = MapReduce(RECORDREADER, MAP, REDUCE)\n",
        "output = list(output)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03IffTEOJgOb"
      },
      "source": [
        "## Вычисление TF-IDF (Term Frequency – Inverse Document Fraquency)\n",
        "\n",
        "Реализуется в три этапа:\n",
        "\n",
        "**Этап 1:** Частота слова в документе\n",
        "\n",
        "**Этап 2:** Количество документов, в которых встречается слово\n",
        "\n",
        "**Этап 3:** Расчёт TF-IDF\n",
        "\n",
        "Реализация включает:\n",
        "1. Формирование набора исходных данных: проиндексированные строки, содержащие текст аннотаций.\n",
        "2. Код функций RECORDREADER_1, MAP_1, REDUCE_1 для этапа 1.\n",
        "3. Код функций RECORDREADER_2, MAP_2, REDUCE_2 для этапа 2.\n",
        "4. Код функций RECORDREADER_3, MAP_3, REDUCE_3 для этапа 3.\n",
        "5. Последовательный вызов модельной функции Mapreduce для всех трёх этапов.\n",
        "6. Ввод на печать результата отдельно для каждого документа в виде первых пяти слов и их TF-IDF, упорядоченных по убыванию их значений (для каждого документа вывести на печать первые пять слов с максимальным TF-IDF)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Исходные данные (аннотации)"
      ],
      "metadata": {
        "id": "EghphM-d1TzX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lGbKpkxGW2V"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "d1 = \"\"\"Streaming data is the data from sensors as well as other real-Rme surveillance systems.\n",
        "Distributed stream processing systems are the soYware that manages such data. Such\n",
        "frameworks have to deliver outcomes on the go instantly. They are suscepRble to delay and\n",
        "malfuncRon or system failures. The system must be tolerant of faults and always accessible.\n",
        "Many variables, such as improved network arrival rates, node failures, and so on, disrupt the\n",
        "system's reliability. Some operators need to be relocated online from one physical resource to\n",
        "another to manage or reimburse a slow or failing node. In this study, we propose a co-locaRon\n",
        "based systemaRc migraRon heurisRc for live operator migraRon between physical resources\n",
        "using a migraRon map revised with costs for each migraRon. The suggested method evaluates\n",
        "conRnuous operator performance pa^erns and makes online scheduling decisions based on the\n",
        "same. The decisions include migraRng operators during a node failure or straggling.\"\"\"\n",
        "\n",
        "d2 = \"\"\"Distributed stream processing engines are designed with a focus on scalability to process big\n",
        "data volumes in a conRnuous manner. We present the Theodolite method for benchmarking the\n",
        "scalability of distributed stream processing engines. Core of this method is the definiRon of use\n",
        "cases that microservices implemenRng stream processing have to fulfill. For each use case, our\n",
        "method idenRfies relevant workload dimensions that might affect the scalability of a use case.\n",
        "We propose to design one benchmark per use case and relevant workload dimension. We\n",
        "present a general benchmarking framework, which can be applied to execute the individual\n",
        "benchmarks for a given use case and workload dimension. Our framework executes an\n",
        "implementaRon of the use case's dataflow architecture for different workloads of the given\n",
        "dimension and various numbers of processing instances. This way, it idenRfies how resources\n",
        "demand evolves with increasing workloads. Within the scope of this paper, we present 4\n",
        "idenRfied use cases, derived from processing Industrial Internet of Things data, and 7\n",
        "corresponding workload dimensions. We provide implementaRons of 4 benchmarks with Kaja\n",
        "Streams and Apache Flink as well as an implementaRon of our benchmarking framework to\n",
        "execute scalability benchmarks in cloud environments. We use both for evaluaRng the\n",
        "Theodolite method and for benchmarking Kaja Streams' and Flink's scalability for different\n",
        "deployment opRons.\"\"\"\n",
        "\n",
        "d3 = \"\"\"Batch and stream processing are separately and efficiently applied in many applicaRons.\n",
        "However, some newer data-driven applicaRons such as the Internet of Things and cloud\n",
        "compuRng call for hybrid processing approaches in order to handle the speed and accuracy\n",
        "required for processing such complex data. In this paper, we propose a Hybrid Distributed\n",
        "Batch-Stream (HDBS) architecture for anomaly detecRon in real-Rme data. The hybrid\n",
        "architecture, while benefiRng from the accuracy provided by batch processing, also enjoys the\n",
        "speed and real-Rme features of stream processing. In the proposed architecture, our focus is on\n",
        "the algorithmic aspects of hybrid processing including the interacRon models between batch\n",
        "and stream processing units, the characterisRcs of batch and stream machine learning\n",
        "algorithms and the principles of merging the results of different processing units. The driving\n",
        "idea of such combinaRon is that the results of batch and stream processing units are\n",
        "complementary with each other, as one of them constructs accurate models based on previous\n",
        "data, and the other one is capable of processing new stream data in real-Rme. Furthermore, we\n",
        "propose a generalized version of the HDBS with respect to its algorithms and communicaRon\n",
        "policy levels. In the generalized HDBS architecture, we address the various aspects of the\n",
        "interacRon between the batch and stream processing units, and the merging operaRons to\n",
        "produce the final results. the evaluaRons of the proposed architecture using various criteria\n",
        "(accuracy, space complexity, and Rme complexity) demonstrate that the accuracy of the\n",
        "proposed method is higher than the accuracy of the batch processing methods, its Rme\n",
        "complexity is also similar to one of the stream processing methods and much less than the\n",
        "batch processing methods, which makes our proposed architecture an efficient and pracRcal\n",
        "soluRon for real-Rme anomaly detecRon.\"\"\"\n",
        "\n",
        "d4 = \"\"\"There have been increasing demands for real Rme processing of the ever-growing data. In order\n",
        "to meet this requirement and ensure the reliable processing of streaming data, a variety of\n",
        "distributed stream processing architectures and plauorms have been developed, which handles\n",
        "the fundamental task of allocaRng processing tasks to the currently available physical resources\n",
        "and rouRng streaming data between these resources. However, many stream processing\n",
        "systems lack an intelligent scheduling mechanism, in which their default schedulers allocate\n",
        "tasks without taking resource demands and availability, or the transfer latency between\n",
        "resources into consideraRon. Besides, stream processing has a strict request for latency. Thus it\n",
        "is important to give latency guarantee for distributed stream processing. In this paper, we\n",
        "propose two new algorithms for stream processing with latency guarantee, both the algorithms\n",
        "consider transfer latency and resource demand in task allocaRon. Both algorithms can\n",
        "guarantee latency constraints. Algorithm AHA reduces more than 21.3% and 58.9% resources\n",
        "compared with the greedy and the round-robin algorithms, and algorithm PHA further improves\n",
        "the resource uRlizaRon to 32.1% and 73.2%.\"\"\"\n",
        "\n",
        "d5 = \"\"\"In the era of Big Data, typical architecture of distributed real-Rme stream processing systems is\n",
        "the combinaRon of Flume, Kaja, and Storm. As a kind of distributed message system, Kaja has\n",
        "the characterisRcs of horizontal scalability and high throughput, which is manly deployed in\n",
        "many areas in order to address the problem of speed mismatch between message producers\n",
        "and consumers. When using Kaja, we need to quickly receive data sent by producers. In\n",
        "addiRon, we need to send data to consumers quickly. Therefore, the performance of Kaja is of\n",
        "criRcal importance to the performance of the whole stream processing system. In this paper, we\n",
        "propose the improved design of real-Rme stream processing systems, and focus on improving\n",
        "the Kaja’s data loading process. We use Kaja cat to transfer data from the source to Kaja\n",
        "topic directly, which can reduce the network transmission. We also uRlize the memory file\n",
        "system to accelerate the process of data loading, which can address the bo^leneck and\n",
        "performance problems caused by disk I/O. Extensive experiments are conducted to evaluate the\n",
        "performance, which show the superiority of our improved design.\"\"\"\n",
        "\n",
        "d6 = \"\"\"In this paper, nearly 40 commonly used deep neural network(DNN) models are selected, and\n",
        "their cross-plauorm and cross-inference frameworks are deeply analysed. The main metrics of\n",
        "accuracy, the total number of model parameters, the computaRonal complexity, the accuracy\n",
        "density, the inference Rme, the memory consumpRon and other related parameters are used to\n",
        "measure their performance. The heterogeneous compuRng experiment is implemented on both\n",
        "the Google Colab cloud compuRng plauorm and the Jetson Nano embedded edge compuRng\n",
        "plauorm. The obtained performance is compared with that of two previous compuRng\n",
        "plauorms: a workstaRon equipped with an NVIDIA Titan X Pascal and an embedded system\n",
        "based on an NVIDIA Jetson TX1 board. In addiRon, on the Jetson Nano embedded edge\n",
        "compuRng plauorm, different inference frameworks are invesRgated to evaluate the inference\n",
        "efficiency of the DNN models. Regression models are established to characterize the variaRon in\n",
        "the compuRng performance of different DNN classificaRon algorithms so that the inference\n",
        "results of unknown models can be esRmated. ANOVA methods are proposed to quanRfy the\n",
        "differences between models. The experimental results have important guiding significance for\n",
        "the be^er selecRon, deployment and applicaRon of DNN models in pracRce. Codes are available\n",
        "at this h^ps URL h^ps://github.com/Foreverzfy/Model-Test.\"\"\"\n",
        "\n",
        "d7 = \"\"\"Unmanned Aerial Vehicles (UAVs), which can operate autonomously in dynamic and complex\n",
        "environments, are becoming increasingly common. Deep learning techniques for moRon control\n",
        "have recently taken a major qualitaRve step since vision-based inference tasks can be executed\n",
        "directly on edge. The goal is to fully integrate the machine learning (ML) element into small\n",
        "UAVs. However, given the limited payload capacity and energy available on small UAVs,\n",
        "integraRng compuRng resources sufficient to host ML and vehicle control funcRons is sRll\n",
        "challenging. This paper presents a modular and generic system that can control the UAV by\n",
        "evaluaRng vision-based ML tasks directly inside the resource-constrained UAV. Two different\n",
        "vision-based navigaRon configuraRons were tested and demonstrated. The first configuraRon\n",
        "implements an autonomous landing site detecRon system, tested with two models based on\n",
        "LeNet-5 and MobileNetV2, respecRvely. This allows the UAV to change its planned path\n",
        "accordingly and approach the target to land. Moreover, a model for people detecRon based on\n",
        "a custom MobileNetV2 network was evaluated in the second configuraRon. Finally, the\n",
        "execuRon Rme and power consumpRon were measured and compared with a cloud compuRng\n",
        "approach. The results show the ability of the developed system to dynamically react to the\n",
        "environment to provide the necessary maneuver aYer detecRng the target exploiRng only the\n",
        "constrained computaRonal resources of the UAV controller. Furthermore, we demonstrated that\n",
        "moving to the edge, instead of using cloud compuRng inference, decreases the energy\n",
        "requirement of the system without reducing the quality of service.\"\"\"\n",
        "\n",
        "d8 = \"\"\"With the conRnuous development of Internet of Things (IoT) and the overwhelming explosion\n",
        "of Big Data, edge compuRng serves as an efficient compuRng mode for Rme stringent data\n",
        "processing, which can bypass the constraints of network bandwidth and delay, and has been\n",
        "one of the foundaRon of interconnected applicaRons. Although edge compuRng has gradually\n",
        "become one of bridges between cloud compuRng centers and mobile terminals, the literature\n",
        "sRll lacks a thorough review on the recent advances in edge compuRng plauorms. In this paper,\n",
        "we firstly introduce the definiRon of edge compuRng and advantages of edge compuRng\n",
        "plauorm. And then, we summarize the key technologies of construcRng an edge compuRng\n",
        "plauorm, and propose a general framework for edge compuRng plauorm. The role of\n",
        "distributed storage management systems in building edge compuRng plauorm is elaborated in\n",
        "detail. Furthermore, we give some applicaRons to illustrate how to use third-party edge\n",
        "compuRng plauorms to build specific applicaRons. Finally, we briefly outline current open issues\n",
        "of edge compuRng plauorm based on our literature survey.\"\"\"\n",
        "\n",
        "corp_doc = [d1, d2, d3, d4, d5, d6, d7, d8]\n",
        "N = len(corp_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Код функций 1-го этапа (с вызовом MapReduce)"
      ],
      "metadata": {
        "id": "TH-gFn5Q1tvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_word(word: str) -> str:\n",
        "  \"\"\"Вспомогательная ф-ция для удаления лишних символов в словах\"\"\"\n",
        "  word = word.removesuffix(\".\")\n",
        "  word = word.removesuffix(\",\")\n",
        "  word = word.removesuffix(\")\")\n",
        "  word = word.removeprefix(\"(\")\n",
        "  return word.lower()"
      ],
      "metadata": {
        "id": "qd-4ptgubTo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RECORDREADER_1():\n",
        "  for docname, content in enumerate(corp_doc):\n",
        "    yield (docname+1, content)\n",
        "\n",
        "def MAP_1(docname: int, content: str):\n",
        "  words = content.split()\n",
        "  n = len(words)\n",
        "  for word in words:\n",
        "    yield ((process_word(word), docname), 1/n)\n",
        "\n",
        "def REDUCE_1(key: tuple[str, int], values: Iterator[float]):\n",
        "  tf = 0\n",
        "  for value in values:\n",
        "    tf += value\n",
        "  yield (key, tf)\n",
        "\n",
        "OUTPUT_1 = MapReduce(RECORDREADER_1, MAP_1, REDUCE_1)"
      ],
      "metadata": {
        "id": "oRrKEG_hIG4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Код функций 2-го этапа (с вызовом MapReduce)"
      ],
      "metadata": {
        "id": "_5JjKXBm13Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RECORDREADER_2():\n",
        "  return OUTPUT_1\n",
        "\n",
        "def MAP_2(key: tuple[str, int], tf: float):\n",
        "  word, docname = key\n",
        "  yield (word, (docname, tf, 1))\n",
        "\n",
        "def REDUCE_2(word: str, values: Iterator[tuple[int, float, int]]):\n",
        "  n = 0\n",
        "  for _, _, one in values:\n",
        "    n += one\n",
        "  for docname, tf, _ in values:\n",
        "    yield ((word, docname), (tf, n))\n",
        "\n",
        "OUTPUT_2 = MapReduce(RECORDREADER_2, MAP_2, REDUCE_2)"
      ],
      "metadata": {
        "id": "-Vrphv7BQMVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Код функций 3-го этапа (с вызовом MapReduce)"
      ],
      "metadata": {
        "id": "Ml5Vtv702C6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RECORDREADER_3():\n",
        "  return OUTPUT_2\n",
        "\n",
        "def MAP_3(key: tuple[str, int], value: tuple[float, int]):\n",
        "  tf, n = value\n",
        "  idf = math.log(N/n, 2)\n",
        "  yield (key, tf*idf)\n",
        "\n",
        "def REDUCE_3(key: tuple[str, int], values: Iterator[float]):\n",
        "  for tf_idf in values:\n",
        "    yield (key, tf_idf)\n",
        "\n",
        "OUTPUT_3 = MapReduce(RECORDREADER_3, MAP_3, REDUCE_3)"
      ],
      "metadata": {
        "id": "5oHfQR2xVjtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод результатов\n",
        "Для удобства сортировки и фильтрации также используется MapReduce."
      ],
      "metadata": {
        "id": "RULgtreh2E2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RECORDREADER_OUTPUT():\n",
        "  return OUTPUT_3\n",
        "\n",
        "def MAP_OUTPUT(key: tuple[str, int], tf_idf: float):\n",
        "  word, docname = key\n",
        "  yield (docname, (word, tf_idf))\n",
        "\n",
        "def REDUCE_OUTPUT(docname: int, values: Iterator[tuple[str, float]]):\n",
        "  docs = sorted(values, key=lambda t: t[1], reverse=True)[:5]\n",
        "  yield (docname, docs)\n",
        "\n",
        "OUTPUT = list(MapReduce(RECORDREADER_OUTPUT, MAP_OUTPUT, REDUCE_OUTPUT))\n",
        "for docname, docs in sorted(OUTPUT):\n",
        "  print(f\"========== Document {docname} ==========\")\n",
        "  for word, tf_idf in docs:\n",
        "    print(f\"{word} - {tf_idf}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDahIW1onRuZ",
        "outputId": "7d35298f-6a17-4df1-d2ec-b77def5ffc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Document 1 ==========\n",
            "migraron - 0.08\n",
            "node - 0.06\n",
            "or - 0.05333333333333334\n",
            "such - 0.04\n",
            "failures - 0.04\n",
            "\n",
            "========== Document 2 ==========\n",
            "benchmarking - 0.05581395348837209\n",
            "case - 0.05581395348837209\n",
            "workload - 0.05581395348837209\n",
            "use - 0.05265255811270116\n",
            "scalability - 0.046511627906976744\n",
            "\n",
            "========== Document 3 ==========\n",
            "batch - 0.08333333333333334\n",
            "hybrid - 0.041666666666666664\n",
            "units - 0.041666666666666664\n",
            "accuracy - 0.034722222222222224\n",
            "hdbs - 0.03125\n",
            "\n",
            "========== Document 4 ==========\n",
            "latency - 0.10344827586206896\n",
            "guarantee - 0.05172413793103448\n",
            "resource - 0.034482758620689655\n",
            "demands - 0.034482758620689655\n",
            "task - 0.034482758620689655\n",
            "\n",
            "========== Document 5 ==========\n",
            "kaja - 0.06521739130434782\n",
            "message - 0.03260869565217391\n",
            "producers - 0.03260869565217391\n",
            "consumers - 0.03260869565217391\n",
            "quickly - 0.03260869565217391\n",
            "\n",
            "========== Document 6 ==========\n",
            "jetson - 0.045454545454545456\n",
            "embedded - 0.045454545454545456\n",
            "dnn - 0.045454545454545456\n",
            "models - 0.04287992422057103\n",
            "inference - 0.04040404040404041\n",
            "\n",
            "========== Document 7 ==========\n",
            "uav - 0.04938271604938272\n",
            "uavs - 0.03703703703703704\n",
            "control - 0.03703703703703704\n",
            "vision-based - 0.03703703703703704\n",
            "ml - 0.03703703703703704\n",
            "\n",
            "========== Document 8 ==========\n",
            "edge - 0.08373002954312686\n",
            "compurng - 0.07100591715976333\n",
            "plauorm - 0.05917159763313609\n",
            "applicarons - 0.03550295857988166\n",
            "literature - 0.03550295857988166\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vU-nf2OGW2V"
      },
      "source": [
        "## Поиск кратчайшего пути на графе с использованием алгоритма BFS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_Ec7mtGW2V"
      },
      "source": [
        "Реализация включает:\n",
        "1. Формирование набора исходных данных: структура, содержащая списки смежности вершин исходного графа.\n",
        "2. Указание начальной и искомой вершин пути на графе.\n",
        "3. Код функции RECORDREADER, формирующей набор исходных данных для каждой итерации, в том числе и для первой итерации.\n",
        "4. Код функций  MAP и REDUCE для каждой итерации.\n",
        "5. Код функции, формирующей признак завершения итераций.\n",
        "6. Код цикла итераций алгоритма BFS, с вызовом модельной функции Mapreduce в теле цикла.\n",
        "7. Вывод на печать кратчайшего пути на графе от исходной до искомой вершины."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Исходные данные (списки смежности)"
      ],
      "metadata": {
        "id": "nhN7VdHd2iqk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaZmdpK9GW2V"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "  def __init__(self, d: int|float, way_list: list[int], adjacency_list: list[int]=None):\n",
        "    self.adjacency_list = adjacency_list\n",
        "    self.d = d\n",
        "    self.way_list = way_list\n",
        "\n",
        "adjacency_dict = {\n",
        "    0: [1, 3],\n",
        "    1: [0, 2, 4],\n",
        "    2: [1, 3, 5, 6],\n",
        "    3: [0, 2, 8],\n",
        "    4: [1, 10, 11],\n",
        "    5: [2, 8, 10],\n",
        "    6: [2, 7],\n",
        "    7: [6, 12],\n",
        "    8: [3, 5, 7, 9],\n",
        "    9: [8, 10],\n",
        "    10: [4, 5, 9, 11, 16],\n",
        "    11: [4, 10, 13],\n",
        "    12: [7, 14],\n",
        "    13: [11, 14],\n",
        "    14: [12, 13, 15],\n",
        "    15: [14, 16],\n",
        "    16: [10, 15, 17],\n",
        "    17: [16, 18],\n",
        "    18: [17, 19],\n",
        "    19: [18]\n",
        "}\n",
        "\n",
        "start_node = 0\n",
        "end_node = 19\n",
        "\n",
        "INPUT_NODES = [(0, Node(0, []))]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Код функций MapReduce алгоритма BFS"
      ],
      "metadata": {
        "id": "NQVcn8Kg3Ghk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RECORDREADER_BFS():\n",
        "  for value in INPUT_NODES:\n",
        "    if value[1].adjacency_list is None:\n",
        "      value[1].adjacency_list = adjacency_dict[value[0]]\n",
        "    yield value\n",
        "\n",
        "def MAP_BFS(n: int, N: Node):\n",
        "  yield (n, N)\n",
        "  for nid in N.adjacency_list:\n",
        "    yield (nid, (N.d+1, N.way_list+[n]))\n",
        "\n",
        "def REDUCE_BFS(n: int, values: Iterator):\n",
        "  dmin = math.inf\n",
        "  way_list = None\n",
        "  N = None\n",
        "  for d in values:\n",
        "    if isinstance(d, Node):\n",
        "      N = d\n",
        "    elif d[0] < dmin:\n",
        "      dmin = d[0]\n",
        "      way_list = d[1]\n",
        "  yield (n, Node(dmin, way_list) if N is None else N)"
      ],
      "metadata": {
        "id": "m78bOkuFA90v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Цикл итераций алгоритма BFS\n",
        "В коде функции останова проверяется число пройденных узлов."
      ],
      "metadata": {
        "id": "RWl299tx3sDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def need_continue():\n",
        "  return len(INPUT_NODES) < len(adjacency_dict)"
      ],
      "metadata": {
        "id": "2Wd9JQtY4jh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while need_continue():\n",
        "  INPUT_NODES = list(MapReduce(RECORDREADER_BFS, MAP_BFS, REDUCE_BFS))"
      ],
      "metadata": {
        "id": "eN6_-Zd8B78m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MapReduce(RECORDREADER_BFS, MAP_BFS, REDUCE_BFS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VRKlyEJ7cEh",
        "outputId": "10e06608-a0e8-4ccf-f77b-550e54e93f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object flatten at 0x784d4467d2f0>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод результатов"
      ],
      "metadata": {
        "id": "FwMhIGnN3YH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_WAY = list(filter(lambda t: t[0] == 19, INPUT_NODES))[0]\n",
        "print(\"====== Way from 0 to 19 ======\")\n",
        "print(OUTPUT_WAY[1].way_list + [OUTPUT_WAY[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlxFq0jLBmMo",
        "outputId": "89a33fd0-f5cd-4862-8fc7-52905d6ac384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====== Way from 0 to 19 ======\n",
            "[0, 1, 4, 10, 16, 17, 18, 19]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}